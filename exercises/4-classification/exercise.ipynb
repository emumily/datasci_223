{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification on `emnist`\n",
    "\n",
    "## 1. Create `Readme.md` to document your work\n",
    "\n",
    "Explain your choices, process, and outcomes.\n",
    "\n",
    "## 2. Classify all symbols\n",
    "\n",
    "### Choose a model\n",
    "\n",
    "Your choice of model! Choose wisely...\n",
    "\n",
    "### Train away!\n",
    "\n",
    "Is do you need to tune any parameters? Is the model expecting data in a different format?\n",
    "\n",
    "### Evaluate the model\n",
    "\n",
    "Evaluate the models on the test set, analyze the confusion matrix to see where the model performs well and where it struggles.\n",
    "\n",
    "### Investigate subsets\n",
    "\n",
    "On which classes does the model perform well? Poorly? Evaluate again, excluding easily confused symbols (such as 'O' and '0').\n",
    "\n",
    "### Improve performance\n",
    "\n",
    "Brainstorm for improving the performance. This could include trying different architectures, adding more layers, changing the loss function, or using data augmentation techniques.\n",
    "\n",
    "## 2. Classify digits vs. letters model showdown\n",
    "\n",
    "Perform a full showdown classifying digits vs letters:\n",
    "\n",
    "1. Create a column for whether each row is a digit or a letter\n",
    "2. Choose an evaluation metric \n",
    "3. Choose several candidate models to train\n",
    "4. Divide data to reserve a validation set that will NOT be used in training/testing\n",
    "5. K-fold train/test\n",
    "    1. Create train/test splits from the non-validation dataset \n",
    "    2. Train each candidate model (best practice: use the same split for all models)\n",
    "    3. Apply the model the the test split \n",
    "    4. (*Optional*) Perform hyper-parametric search\n",
    "    5. Record the model evaluation metrics\n",
    "    6. Repeat with a new train/test split\n",
    "6. Promote winner, apply model to validation set\n",
    "7. (*Optional*) Perform hyper-parametric search, if applicable\n",
    "8. Report model performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.4.0-1-cp311-cp311-macosx_10_9_x86_64.whl.metadata (11 kB)\n",
      "Collecting xgboost\n",
      "  Downloading xgboost-2.0.3-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in /Users/emilytang/Desktop/UCSF_TA/ucsf_ds_ta/.my_env3/lib/python3.11/site-packages (from scikit-learn) (1.26.3)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Using cached scipy-1.12.0-cp311-cp311-macosx_10_9_x86_64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.3.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.2.0-py3-none-any.whl.metadata (10.0 kB)\n",
      "Using cached scikit_learn-1.4.0-1-cp311-cp311-macosx_10_9_x86_64.whl (11.5 MB)\n",
      "Downloading xgboost-2.0.3-py3-none-macosx_10_15_x86_64.macosx_11_0_x86_64.macosx_12_0_x86_64.whl (2.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[?25hUsing cached joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "Using cached scipy-1.12.0-cp311-cp311-macosx_10_9_x86_64.whl (38.9 MB)\n",
      "Using cached threadpoolctl-3.2.0-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, xgboost, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.0 scipy-1.12.0 threadpoolctl-3.2.0 xgboost-2.0.3\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import emnist\n",
    "from hashlib import sha1\n",
    "\n",
    "%pip install scikit-learn xgboost\n",
    "\n",
    "# Import necessary libraries for classification\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data, and reshape it into a 28x28 array\n",
    "\n",
    "# The size of each image is 28x28 pixels\n",
    "size = 28\n",
    "\n",
    "# Extract the training split as images and labels\n",
    "image, label = emnist.extract_training_samples('byclass')\n",
    "\n",
    "# Add columns for each pixel value (28x28 = 784 columns)\n",
    "raw_train = pd.DataFrame()\n",
    "\n",
    "# Add a column showing the label\n",
    "raw_train['label'] = label\n",
    "\n",
    "# Add a column with the image data as a 28x28 array\n",
    "raw_train['image'] = list(image)\n",
    "\n",
    "\n",
    "# Repeat for the test split\n",
    "image, label = emnist.extract_test_samples('byclass')\n",
    "raw_test = pd.DataFrame()\n",
    "raw_test['label'] = label\n",
    "raw_test['image'] = list(image)\n",
    "\n",
    "merged = pd.concat([raw_test, raw_train], axis=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Assuming X contains your features and y contains your labels\n",
    "# Replace this with your actual data loading/preprocessing code\n",
    "X = merged.drop(\"label\", axis=1)  # Assuming 'label' is the column you want to predict\n",
    "y = merged[\"label\"]\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Decision Tree Classifier\n",
    "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# When simply training the model\n",
    "# # Train the model\n",
    "# decision_tree_model.fit(X_train, y_train)\n",
    "\n",
    "# # Make predictions on the test set\n",
    "# y_pred = decision_tree_model.predict(X_test)\n",
    "\n",
    "# When you want to do hyperparameter tuning\n",
    "# Define hyperparameters to tune\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "# Use GridSearchCV for hyperparameter tuning\n",
    "grid_search = GridSearchCV(estimator=decision_tree_model, param_grid=param_grid, cv=3, scoring='f1_weighted')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best model from hyperparameter tuning\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model using F1 score and confusion matrix\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Recategorizing the labels into 'numbers' and 'letters' (from HW Soln 2)\n",
    "\n",
    "# Using range(10) to identify numbers and range(10, 62) to identify letters\n",
    "merged['cat_lambda'] = merged['label'].apply(lambda x: 'numbers' if x in range(10) else 'letters')\n",
    "\n",
    "# Using a function to identify numbers and letters\n",
    "def classify_label(x):\n",
    "    if x in range(10):\n",
    "        return 'numbers'\n",
    "    elif x in range(10, 62):\n",
    "        return 'letters'\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "merged['cat_label'] = merged['label'].apply(classify_label)\n",
    "\n",
    "# # Using direct if statements to identify numbers and letters\n",
    "# merged['cat_if'] = ['numbers' if x in range(10) else 'letters' if x in range(10, 62) else None for x in merged['label']]\n",
    "\n",
    "# # Using pandas filters to identify numbers and letters, with the .loc() method to filter the rows\n",
    "# merged['cat_filter'] = None\n",
    "# merged.loc[merged['label'].isin(range(10)), 'cat_filter'] = 'numbers'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Create a validation set that will NOT be used in training/testing\n",
    "X = merged.drop(\"cat_label\", axis=1)  # Assuming 'cat_label' is the column you want to predict\n",
    "y = merged[\"cat_label\"]\n",
    "\n",
    "# 20% for test (unseen), 80% split into 80 and 20 again, for train validation respectively\n",
    "X_train_test, X_validation, y_train_test, y_validation = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Step 3: K-fold train/test\n",
    "# Initialize models\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "xgb_model = XGBClassifier(random_state=42)\n",
    "lr_model = LogisticRegression(random_state=42)\n",
    "\n",
    "# Combine models into a list for iteration\n",
    "models = [(\"Random Forest\", rf_model), (\"XGBoost\", xgb_model), (\"Logistic Regression\", lr_model)]\n",
    "\n",
    "# Initialize StratifiedKFold for stratified K-fold cross-validation\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize dictionaries to store model performance metrics\n",
    "model_metrics = {\"Random Forest\": [], \"XGBoost\": [], \"Logistic Regression\": []}\n",
    "\n",
    "# Step 3.1 to 3.5: K-fold cross-validation\n",
    "for model_name, model in models:\n",
    "    print(f\"Training and evaluating {model_name}...\")\n",
    "\n",
    "    for train_idx, test_idx in stratified_kfold.split(X_train_test):\n",
    "        X_train, X_test = X_train_test.iloc[train_idx], X_train_test.iloc[test_idx]\n",
    "        y_train, y_test = y_train_test.iloc[train_idx], y_train_test.iloc[test_idx]\n",
    "\n",
    "        # Train the model\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the test split\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Record the model evaluation metrics\n",
    "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "        model_metrics[model_name].append(f1)\n",
    "\n",
    "# Step 4: Promote winner\n",
    "best_model_name = max(model_metrics, key=lambda k: np.mean(model_metrics[k]))\n",
    "best_model = next(model for model_name, model in models if model_name == best_model_name)\n",
    "\n",
    "# Step 5: Apply the best model to the validation set\n",
    "best_model.fit(X_train_test, y_train_test)\n",
    "y_validation_pred = best_model.predict(X_validation)\n",
    "\n",
    "# Step 6: Report model performance (confusion matrix)\n",
    "conf_matrix = confusion_matrix(y_validation, y_validation_pred)\n",
    "\n",
    "# Step 7: Print results\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_validation, y_validation_pred))\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".my_env3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
