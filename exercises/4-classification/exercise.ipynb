{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTW0ngoqm_mW"
      },
      "source": [
        "# Classification on `emnist`\n",
        "\n",
        "## 1. Create `Readme.md` to document your work\n",
        "\n",
        "Explain your choices, process, and outcomes.\n",
        "\n",
        "## 2. Classify all symbols\n",
        "\n",
        "### Choose a model\n",
        "\n",
        "Your choice of model! Choose wisely...\n",
        "\n",
        "### Train away!\n",
        "\n",
        "Is do you need to tune any parameters? Is the model expecting data in a different format?\n",
        "\n",
        "### Evaluate the model\n",
        "\n",
        "Evaluate the models on the test set, analyze the confusion matrix to see where the model performs well and where it struggles.\n",
        "\n",
        "### Investigate subsets\n",
        "\n",
        "On which classes does the model perform well? Poorly? Evaluate again, excluding easily confused symbols (such as 'O' and '0').\n",
        "\n",
        "### Improve performance\n",
        "\n",
        "Brainstorm for improving the performance. This could include trying different architectures, adding more layers, changing the loss function, or using data augmentation techniques.\n",
        "\n",
        "## 2. Classify digits vs. letters model showdown\n",
        "\n",
        "Perform a full showdown classifying digits vs letters:\n",
        "\n",
        "1. Create a column for whether each row is a digit or a letter\n",
        "2. Choose an evaluation metric\n",
        "3. Choose several candidate models to train\n",
        "4. Divide data to reserve a validation set that will NOT be used in training/testing\n",
        "5. K-fold train/test\n",
        "    1. Create train/test splits from the non-validation dataset\n",
        "    2. Train each candidate model (best practice: use the same split for all models)\n",
        "    3. Apply the model the the test split\n",
        "    4. (*Optional*) Perform hyper-parametric search\n",
        "    5. Record the model evaluation metrics\n",
        "    6. Repeat with a new train/test split\n",
        "6. Promote winner, apply model to validation set\n",
        "7. (*Optional*) Perform hyper-parametric search, if applicable\n",
        "8. Report model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8d6mMrvm_mb"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture captured_output1\n",
        "\n",
        "# Code producing output for cell 1\n",
        "captured_output1=pd.DataFrame()\n",
        "\n",
        "with open('output.txt', 'a') as f:\n",
        "    f.write(captured_output1.stdout)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JLl2Go3PA60N",
        "outputId": "0c248f0e-eff2-42ec-a363-3020f117fcf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute 'stdout'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-417f01778dda>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'output.txt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'a'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcaptured_output1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'stdout'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture captured_output2\n",
        "\n",
        "\n",
        "# Code producing output for cell 2\n",
        "\n",
        "with open('output.txt', 'a') as f:\n",
        "    f.write(captured_output2.stdout)"
      ],
      "metadata": {
        "id": "U2w_ISiNBF-I",
        "outputId": "b2e55685-3c71-4f05-86aa-939d8bb0686e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emnist\n",
            "  Downloading emnist-0.0-py3-none-any.whl (7.3 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from emnist) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from emnist) (2.31.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from emnist) (4.66.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->emnist) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->emnist) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->emnist) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->emnist) (2024.2.2)\n",
            "Installing collected packages: emnist\n",
            "Successfully installed emnist-0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "WlVBAYGQm_mc",
        "outputId": "92ed3ddf-4fb9-4190-cecc-be03f9c17f31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.2.0)\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import emnist\n",
        "from hashlib import sha1\n",
        "\n",
        "%pip install scikit-learn xgboost\n",
        "\n",
        "# Import necessary libraries for classification\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, classification_report, confusion_matrix\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jx6bGLmem_me"
      },
      "outputs": [],
      "source": [
        "# Load the data, and reshape it into a 28x28 array\n",
        "\n",
        "# The size of each image is 28x28 pixels\n",
        "size = 28\n",
        "\n",
        "# Extract the training split as images and labels\n",
        "image, label = emnist.extract_training_samples('byclass')\n",
        "\n",
        "# Add columns for each pixel value (28x28 = 784 columns)\n",
        "raw_train = pd.DataFrame()\n",
        "\n",
        "# Add a column showing the label\n",
        "raw_train['label'] = label\n",
        "\n",
        "# Add a column with the image data as a 28x28 array\n",
        "raw_train['image'] = list(image)\n",
        "\n",
        "\n",
        "# Repeat for the test split\n",
        "image, label = emnist.extract_test_samples('byclass')\n",
        "raw_test = pd.DataFrame()\n",
        "raw_test['label'] = label\n",
        "raw_test['image'] = list(image)\n",
        "\n",
        "merged = pd.concat([raw_test, raw_train], axis=0)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RSpZQ4nxm_mg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Assuming X contains your features and y contains your labels\n",
        "# Replace this with your actual data loading/preprocessing code\n",
        "X = merged.drop(\"label\", axis=1)  # Assuming 'label' is the column you want to predict\n",
        "y = merged[\"label\"]\n",
        "\n",
        "# Split the dataset into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create a Decision Tree Classifier\n",
        "decision_tree_model = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "# When simply training the model\n",
        "# # Train the model\n",
        "# decision_tree_model.fit(X_train, y_train)\n",
        "\n",
        "# # Make predictions on the test set\n",
        "# y_pred = decision_tree_model.predict(X_test)\n",
        "\n",
        "# When you want to do hyperparameter tuning\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'criterion': ['gini', 'entropy'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "# Use GridSearchCV for hyperparameter tuning\n",
        "grid_search = GridSearchCV(estimator=decision_tree_model, param_grid=param_grid, cv=3, scoring='f1_weighted')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model from hyperparameter tuning\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = best_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model using F1 score and confusion matrix\n",
        "f1 = f1_score(y_test, y_pred, average=None)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred,average='micro') # same as accuracy here\n",
        "\n",
        "# Print the results\n",
        "print(f\"Best Hyperparameters: {grid_search.best_params_}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qra4Z-sUm_mi"
      },
      "source": [
        "## Part 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JIr0e9kxm_mi"
      },
      "outputs": [],
      "source": [
        "# Step 1: Recategorizing the labels into 'numbers' and 'letters' (from HW Soln 2)\n",
        "\n",
        "# Using range(10) to identify numbers and range(10, 62) to identify letters\n",
        "merged['cat_lambda'] = merged['label'].apply(lambda x: 'numbers' if x in range(10) else 'letters')\n",
        "\n",
        "# Using a function to identify numbers and letters\n",
        "def classify_label(x):\n",
        "    if x in range(10):\n",
        "        return 'numbers'\n",
        "    elif x in range(10, 62):\n",
        "        return 'letters'\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "merged['cat_label'] = merged['label'].apply(classify_label)\n",
        "\n",
        "# # Using direct if statements to identify numbers and letters\n",
        "# merged['cat_if'] = ['numbers' if x in range(10) else 'letters' if x in range(10, 62) else None for x in merged['label']]\n",
        "\n",
        "# # Using pandas filters to identify numbers and letters, with the .loc() method to filter the rows\n",
        "# merged['cat_filter'] = None\n",
        "# merged.loc[merged['label'].isin(range(10)), 'cat_filter'] = 'numbers'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M0oL8ebnm_mj"
      },
      "outputs": [],
      "source": [
        "# Step 2: Create a validation set that will NOT be used in training/testing\n",
        "## X_train!!!!\n",
        "X = merged.drop(\"cat_label\", axis=1)  # Assuming 'cat_label' is the column you want to predict\n",
        "y = merged[\"cat_label\"]\n",
        "\n",
        "# 20% for test (unseen), 80% split into 80 and 20 again, for train validation respectively\n",
        "X_train_test, X_validation, y_train_test, y_validation = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 3: K-fold train/test\n",
        "# Initialize models\n",
        "rf_model = RandomForestClassifier(random_state=42)\n",
        "xgb_model = XGBClassifier(random_state=42)\n",
        "lr_model = LogisticRegression(random_state=42)\n",
        "\n",
        "# Combine models into a list for iteration\n",
        "models = [(\"Random Forest\", rf_model), (\"XGBoost\", xgb_model), (\"Logistic Regression\", lr_model)]\n",
        "\n",
        "# Initialize StratifiedKFold for stratified K-fold cross-validation\n",
        "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize dictionaries to store model performance metrics\n",
        "model_metrics = {\"Random Forest\": [], \"XGBoost\": [], \"Logistic Regression\": []}\n",
        "\n",
        "# Step 3.1 to 3.5: K-fold cross-validation\n",
        "for model_name, model in models:\n",
        "    print(f\"Training and evaluating {model_name}...\")\n",
        "\n",
        "    for train_idx, test_idx in stratified_kfold.split(X_train_test):\n",
        "        X_train, X_test = X_train_test.iloc[train_idx], X_train_test.iloc[test_idx]\n",
        "        y_train, y_test = y_train_test.iloc[train_idx], y_train_test.iloc[test_idx]\n",
        "\n",
        "        # Train the model\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Make predictions on the test split\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Record the model evaluation metrics\n",
        "        f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "        model_metrics[model_name].append(f1)\n",
        "\n",
        "# Step 4: Promote winner\n",
        "best_model_name = max(model_metrics, key=lambda k: np.mean(model_metrics[k]))\n",
        "best_model = next(model for model_name, model in models if model_name == best_model_name)\n",
        "\n",
        "# Step 5: Apply the best model to the validation set\n",
        "best_model.fit(X_train_test, y_train_test)\n",
        "y_validation_pred = best_model.predict(X_validation)\n",
        "\n",
        "# Step 6: Report model performance (confusion matrix)\n",
        "conf_matrix = confusion_matrix(y_validation, y_validation_pred)\n",
        "\n",
        "# Step 7: Print results\n",
        "print(f\"Best Model: {best_model_name}\")\n",
        "print(\"Classification Report:\")\n",
        "print(classification_report(y_validation, y_validation_pred))\n",
        "print(\"Confusion Matrix:\")\n",
        "print(conf_matrix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16gw_oFtm_ml"
      },
      "source": [
        "### Adding tensorflow NN as practice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7LPIEWqVm_mn"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Assuming X contains your features and y contains your labels\n",
        "# Replace this with your actual data loading/preprocessing code\n",
        "# same as above\n",
        "# X = merged.drop(\"cat_label\", axis=1)  # Assuming 'cat_label' is the column you want to predict\n",
        "# y = merged[\"cat_label\"]\n",
        "\n",
        "# Step 1: Create a validation set that will NOT be used in training/testing\n",
        "X_train_test, X_validation, y_train_test, y_validation = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Step 2: K-fold train/test\n",
        "# Define the neural network architecture\n",
        "model = tf.keras.Sequential([\n",
        "    # Add your layers here\n",
        "    tf.keras.layers.Dense(units=64, activation='relu', input_shape=(5,)), # number of features\n",
        "    tf.keras.layers.Dense(units=10, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile the model with an appropriate optimizer, loss function, and metrics\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Initialize KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialize list to store evaluation metrics\n",
        "conf_matrices = []\n",
        "\n",
        "# Step 3.1 to 3.5: K-fold cross-validation\n",
        "for train_idx, test_idx in kfold.split(X_train_test):\n",
        "    X_train, X_test = X_train_test[train_idx], X_train_test[test_idx]\n",
        "    y_train, y_test = y_train_test[train_idx], y_train_test[test_idx]\n",
        "\n",
        "    # Step 3.2: Train the model\n",
        "    model.fit(X_train, y_train, epochs=10, batch_size=32, validation_data=(X_validation, y_validation))\n",
        "\n",
        "    # Step 3.3: Apply the model to the test split\n",
        "    y_pred = model.predict(X_test)\n",
        "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "\n",
        "    # Step 3.4: Record the model evaluation metrics\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
        "    conf_matrices.append(conf_matrix)\n",
        "\n",
        "# Step 4: Report model performance (average confusion matrix)\n",
        "average_conf_matrix = np.mean(conf_matrices, axis=0)\n",
        "print(\"Average Confusion Matrix:\")\n",
        "print(average_conf_matrix)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".my_env3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}